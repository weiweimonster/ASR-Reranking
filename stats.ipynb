{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Vocabulary Size  Avg Sentence Length  Avg Word Length  Total Sentences  \\\n",
      "Train          32215.0           122.694603         4.947067          36261.0   \n",
      "Dev            11084.0           115.984051         4.892171           5204.0   \n",
      "Test           10555.0           117.934505         4.946610           5176.0   \n",
      "\n",
      "       Total Words  \n",
      "Train     754202.0  \n",
      "Dev       103321.0  \n",
      "Test      103522.0  \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Paths setup\n",
    "base_path = 'data'\n",
    "wsj_path = os.path.join(base_path, 'lm_data')\n",
    "wer_path = os.path.join(base_path, 'wer_data')\n",
    "\n",
    "wsj_train_path = os.path.join(wsj_path, 'treebank-sentences-train.txt')\n",
    "wsj_dev_path = os.path.join(wsj_path, 'treebank-sentences-dev.txt')\n",
    "wsj_test_path = os.path.join(wsj_path, 'treebank-sentences-test.txt')\n",
    "wer_dev_path = os.path.join(wer_path, 'dev_sentences.json')\n",
    "wer_test_path = os.path.join(wer_path, 'test_sentences.json')\n",
    "\n",
    "# Load WSJ data\n",
    "def load_wsj_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().splitlines()\n",
    "    return data\n",
    "\n",
    "# Load WER data\n",
    "def load_wer_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    sentences = []\n",
    "    for item in data.values():\n",
    "        sentences.extend(item['sentences'])\n",
    "    return sentences\n",
    "\n",
    "# Function to calculate statistics\n",
    "def calculate_statistics(data):\n",
    "    words = ' '.join(data).split()\n",
    "    vocab = Counter(words)\n",
    "    vocab_size = len(vocab)\n",
    "    avg_sentence_length = sum(map(len, data)) / len(data)\n",
    "    avg_word_length = sum(map(len, words)) / len(words) if words else 0\n",
    "    return vocab_size, avg_sentence_length, avg_word_length, len(data), len(words)\n",
    "\n",
    "# Load data\n",
    "datasets = {\n",
    "    \"Train\": load_wsj_data(wsj_train_path),\n",
    "    \"Dev\": load_wsj_data(wsj_dev_path) + load_wer_data(wer_dev_path),\n",
    "    \"Test\": load_wsj_data(wsj_test_path) + load_wer_data(wer_test_path)\n",
    "}\n",
    "\n",
    "# Calculate and store statistics for each dataset\n",
    "stats = {}\n",
    "for name, data in datasets.items():\n",
    "    stats[name] = calculate_statistics(data)\n",
    "\n",
    "# Convert statistics to DataFrame for display\n",
    "df_stats = pd.DataFrame(stats, index=[\"Vocabulary Size\", \"Avg Sentence Length\", \"Avg Word Length\", \"Total Sentences\", \"Total Words\"]).T\n",
    "print(df_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dataset  Total Sentences  Total Words  Average Sentence Length  \\\n",
      "0   Train            36261       754202                20.799261   \n",
      "1     Dev             4529        93775                20.705454   \n",
      "2    Test             4554        94230                20.691700   \n",
      "\n",
      "   Average Word Length  Vocabulary Size  \n",
      "0             4.947067            27774  \n",
      "1             4.861829            27774  \n",
      "2             4.875348            27774  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "base_path = 'data/lm_data'\n",
    "wsj_train_path = os.path.join(base_path, 'treebank-sentences-train.txt')\n",
    "wsj_dev_path = os.path.join(base_path, 'treebank-sentences-dev.txt')\n",
    "wsj_test_path = os.path.join(base_path, 'treebank-sentences-test.txt')\n",
    "\n",
    "# Define special tokens\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().splitlines()\n",
    "    # Lowercase and tokenize\n",
    "    data = [line.lower().split() for line in lines]\n",
    "    return data\n",
    "\n",
    "# Function to build vocabulary and handle unknowns\n",
    "def build_vocab(data):\n",
    "    vocab_counter = Counter([token for sentence in data for token in sentence])\n",
    "    vocab = {word: i for i, (word, _) in enumerate(vocab_counter.items(), start=1)}\n",
    "    vocab[UNK_TOKEN] = 0  # Reserve index 0 for UNK\n",
    "    return vocab, vocab_counter\n",
    "\n",
    "# Function to replace unknown tokens\n",
    "def replace_unknowns(data, vocab):\n",
    "    return [[token if token in vocab else UNK_TOKEN for token in sentence] for sentence in data]\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data = load_and_preprocess_data(wsj_train_path)\n",
    "dev_data = load_and_preprocess_data(wsj_dev_path)\n",
    "test_data = load_and_preprocess_data(wsj_test_path)\n",
    "\n",
    "# Build vocab from train data only and get the word count\n",
    "vocab, word_count = build_vocab(train_data)\n",
    "\n",
    "# Replace unknowns in all datasets\n",
    "train_data_processed = replace_unknowns(train_data, vocab)\n",
    "dev_data_processed = replace_unknowns(dev_data, vocab)\n",
    "test_data_processed = replace_unknowns(test_data, vocab)\n",
    "\n",
    "# Calculate statistics\n",
    "def calculate_statistics(data):\n",
    "    total_words = sum(len(sentence) for sentence in data)\n",
    "    total_sentences = len(data)\n",
    "    avg_sentence_length = total_words / total_sentences\n",
    "    word_lengths = [len(word) for sentence in data for word in sentence]\n",
    "    avg_word_length = np.mean(word_lengths)\n",
    "    return total_sentences, total_words, avg_sentence_length, avg_word_length\n",
    "\n",
    "stats = {\n",
    "    \"Dataset\": [\"Train\", \"Dev\", \"Test\"],\n",
    "    \"Total Sentences\": [],\n",
    "    \"Total Words\": [],\n",
    "    \"Average Sentence Length\": [],\n",
    "    \"Average Word Length\": [],\n",
    "    \"Vocabulary Size\": len(vocab) - 1  # Excluding <UNK> token\n",
    "}\n",
    "\n",
    "for dataset in [train_data_processed, dev_data_processed, test_data_processed]:\n",
    "    total_sentences, total_words, avg_sentence_length, avg_word_length = calculate_statistics(dataset)\n",
    "    stats[\"Total Sentences\"].append(total_sentences)\n",
    "    stats[\"Total Words\"].append(total_words)\n",
    "    stats[\"Average Sentence Length\"].append(avg_sentence_length)\n",
    "    stats[\"Average Word Length\"].append(avg_word_length)\n",
    "\n",
    "# Convert stats to DataFrame for display\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(df_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
